{"version":3,"sources":["webpack:///./src/pages/projects.js","webpack:///./src/assets/images/bplant_overlap.png","webpack:///./src/assets/images/bplant_0.png","webpack:///./src/assets/images/chair_1.png","webpack:///./src/assets/images/bplant_2.png","webpack:///./src/assets/images/chair_base.png","webpack:///./src/assets/images/bplant_1.png","webpack:///./src/assets/images/bplant_base.png","webpack:///./src/assets/images/chair_0.png"],"names":["Projects","name","content","id","className","src","chairbase","chair0","chair1","bplantbase","width","bplant_overlap","bplant2","href","bplant1","module","exports"],"mappings":"6FAAA,wQAce,SAASA,IAKtB,OACE,kBAAC,IAAD,KAEE,kBAAC,IAAD,KACE,+BAPc,mBAQd,0BAAMC,KAAK,cAAcC,QAPL,cAUtB,yBAAKC,GAAG,QAGN,kDACA,6BAEA,mGAGA,yBAAKC,UAAU,cACf,yBAAKC,IAAKC,IAAWF,UAAY,UAEjC,6BAAM,6BAZR,oUAiBE,6BAEA,6BAGA,yBAAKA,UAAU,eACf,yBAAKC,IAAKE,IAAQH,UAAY,WAvBhC,4UAgC8B,6BAAK,6BAhCnC,mLA6CE,yBAAKA,UAAU,cACf,yBAAKC,IAAKG,IAAQJ,UAAY,UAE9B,6BAAM,6BAhDR,gVAqDE,6BACA,6BACA,6BACA,6BACA,6BAEA,6BAIA,gFAEA,yBAAKA,UAAU,cACf,yBAAKC,IAAKI,IAAYC,MAAM,MAAMN,UAAY,UAlEhD,oRAyEE,6BAAK,6BAzEP,iFA2EgC,qDA3EhC,+EA4EE,6BAAK,6BA5EP,gfAsFE,6BAAM,6BAEN,yBAAKA,UAAU,gBACV,yBAAKC,IAAKM,IAAgBD,MAAM,SAErC,6BAAM,6BA3FR,+YAgGkD,4EAhGlD,uMAkGE,6BACA,6BACA,yBAAKN,UAAU,gBACV,yBAAKC,IAAKO,IAASF,MAAM,SAE9B,6BAAM,6BAvGR,qNA0GuC,6BAAK,6BA1G5C,qQA8GqB,wCA9GrB,ykBAkHE,6BAAK,6BAlHP,8DAmH6D,mCAnH7D,wcAsHE,uBAAGG,KAAK,eAAR,aAtHF,qDAsHuF,uBAAGA,KAAK,gDAAR,kBAtHvF,gFAsH6O,6BAAM,6BAIjP,yBAAKT,UAAU,gBACV,yBAAKC,IAAKS,IAASJ,MAAM,SAWlC,6BAAM,6BACN,2CAvIE,wJAwImJ,uDAxInJ,SAwIwL,6BAxIxL,4EAyIuE,6FAzIvE,UA0IsD,6BA1ItD,wEA2ImE,mDA3InE,WA2IsG,6BA3ItG,uEA8IE,yHA9IF,SA+IiD,6BA/IjD,oIAkJkB,oEAlJlB,WAmJgB,6BAnJhB,qGAqJyC,mDArJzC,WAqJ4E,6BArJ5E,mIAwJN,6EAxJM,WAwJuD,6BAxJvD,8FA0JwB,2DA1JxB,gBA0JwE,6BAK9E,6BAGQ,4DAlKF,sBAoKqB,uBAAGG,KAAK,uBAAR,YApKrB,gTAyKE,6BAAM,6BAzKR,ujBAgLE,6BAAM,6BAhLR,mdAuLE,6BAAM,6BAvLR,sbA6LE,6BAAM,6BA7LR,mSAiME,6BAAM,6BAjMR,2NAqME,6BACA,6BAtMF,uIA0ME,6BACA,6BACA,0CA5MF,iJA8MyC,oCA9MzC,SA+ME,iC,uBC1ORE,EAAOC,QAAU,IAA0B,8D,qBCA3CD,EAAOC,QAAU,IAA0B,wD,qBCA3CD,EAAOC,QAAU,IAA0B,uD,qBCA3CD,EAAOC,QAAU,IAA0B,wD,qBCA3CD,EAAOC,QAAU,IAA0B,0D,qBCA3CD,EAAOC,QAAU,IAA0B,wD,qBCA3CD,EAAOC,QAAU,IAA0B,2D,qBCA3CD,EAAOC,QAAU,IAA0B","file":"component---src-pages-projects-js-0e064f235ca3a853a48e.js","sourcesContent":["import React from 'react'\nimport Helmet from 'react-helmet'\n\nimport Layout from '../components/layout'\n\nimport bplant_overlap from '../assets/images/bplant_overlap.png'\nimport chairbase from '../assets/images/chair_base.png'\nimport bplantbase from '../assets/images/bplant_base.png'\nimport chair0 from '../assets/images/chair_0.png'\nimport chair1 from '../assets/images/chair_1.png'\nimport bplant0 from '../assets/images/bplant_0.png'\nimport bplant1 from '../assets/images/bplant_1.png'\nimport bplant2 from '../assets/images/bplant_2.png'\n\nexport default function Projects() {\n\n    const siteTitle = 'Rob Cohn\\'s page'\n    const siteDescription = 'Rob Cohn'\n\n  return (\n    <Layout>\n\n      <Helmet>\n        <title>{siteTitle}</title>\n        <meta name=\"description\" content={siteDescription} />        \n      </Helmet>\n\n      <div id=\"main\">\n        \n\n        <h1>Some Past Projects</h1>\n        <hr />\n            \n        <h2>Autonomous Robotic 3D Scanning for Photorealistic Models of Objects</h2>\n\n        \n        <div className=\"image left\">\n        <img src={chairbase} className = \"left\"/>     \n        </div>\n        <br /><br /> \n        In the context of 3D object scanning, the goal is to measure a \n        3D object like the brown chair to the left with a physical scanning device, \n        and then process the data collected to construct a model of the object: an asset\n        which can be used in a variety of digital media such as movies, games, or online shopping interfaces.\n        <br/>\n        \n        <br />\n        \n        \n        <div className=\"image right\">\n        <img src={chair0} className = \"right\"/>     \n        </div>        \n        The first step of this process is to collect data about the object from all necessary perspectives, which might produce\n        a pointcloud like that shown on the right.  \n        \n\n        \n        This is currently done with hand-held scanning devices,\n        making the process labor-intensive and time-consuming, which limits the number of\n        objects companies can scan. <br/><br/>\n        I'm currently working with 3co on developing an autonomous robotic solution\n        that will offer a way to obtain photorealistic 3D scans of physical objects at the push of a button.\n    \n\n        \n        \n        \n{/*         \n        <div className=\"image left\">\n        <img src={chair1} width=\"60%\" className = \"left\"/>     \n        </div> */}\n        \n        <div className=\"image left\">\n        <img src={chair1} className = \"left\"/>     \n        </div> \n        <br /><br />\n        For those interested, I dive into a little more detail about just the surface reconstruction\n        part of the problem in the next section. That is, given a set of range scans in the form of point clouds representing \n        different views of the object, how that can all be neatly integrated into a single coherent mesh like the one on the left.\n\n        <br />        \n        <br />\n        <br />\n        <br />\n        <br />\n        \n        <hr />\n\n\n        \n        <h2>Surface Reconstruction for Point Cloud Sequences</h2>\n        \n        <div className=\"image left\">\n        <img src={bplantbase} width=\"30%\" className = \"left\"/>     \n        </div>\n\n        Consider the problem of constructing a 3D model of this banana plant from a set of point clouds collected\n        by a range scanner from various view points. Ideally, we would like to construct a watertight mesh, whose \n        geometry, texture, and colors are faithful to the real thing. \n        \n        <br/><br/>\n        Here, I'm going to talk about just the geometry\n        part, which is referred to as <i>surface reconstruction</i>, and is a well-studied and arguably \"solved\" problem in the literature [1].\n        <br/><br/>\n        Meshing individual scans in the sequence is easy, since the perspective used to capture each point cloud \n        naturally imposes a lattice structure, which can be directly iterated through, forming triangles when the maximum\n        side length is under some threshold.\n\n        \n\n        \n        The real challenge arises when attempting to merge each individually meshed scan into a single mesh, as there will be\n        overlap in regions that are captured by multiple scans, which must be resolved for the model to be commercially usable:\n        <br /><br />\n\n        <div className=\"image center\">\n             <img src={bplant_overlap} width=\"80%\"></img>\n             </div>\n        <br /><br />\n\n        The literature offers two main ways to tackle this issue. The first is to actually ignore the\n        lattice structure described above, and perform meshing on the entire point cloud containing all measured points\n        directly. While this does throw away information, in some instances it can work well. In particular, variants of Poisson reconstruction [2,3]\n        can achieve extremely accurate results in cases <i>where all surfaces of the object are observed</i>. In cases of completely absent data for certain regions,\n        such as the underside of the leaves in a top-down view of our banana plant, Poisson reconstruction will fail in rather disturbing fashion:\n        <br />\n        <br />        \n        <div className=\"image center\">\n             <img src={bplant2} width=\"80%\"></img>\n             </div>\n        <br /><br />\n\n        These phantom \"bubbles\" are the result of Poisson reconstruction's (elegant) treatment of the problem as the optimization of a watertight surface whose surface was sampled\n        to produce the scanned point clouds. <br/><br />\n\n        The second family of methods in the literature for handling sets of point clouds collected from differing perspectives treats the problem more directly by growing the mesh\n        at a local level, as opposed to optimizing for a global objective.\n        In particular, the <i>Zippering</i> approach of Turk et. al. [4] meshes the scans individually and then performs decimation in overlapping areas.  This would be an ideal solution, except computationally it does not scale well to the higher resolutions 3co works with. To \n        address this issue, Meerits et. al. [5] have reformulated the algorithm to be GPU-friendly, potentially making it computationally viable for higher resolutions. Unfortunately, their code is\n        not publically available for testing, and their results do not appear to construct models at the level of accuracy needed for commercially usable 3D models.\n\n        <br/><br/>\n        In the end, the best approach for this setting seems to be <i>VRIP</i>, contributed by Curless et. al.[6]. VRIP utilizes the lattice structure of \n        each scan's observations, but instead of individually meshing scans and then merging them, VRIP iterates across voxels in a volumetric representation of the object in an efficient\n        manner that accounts for the knowledge of where free space is given the persective of the scanner across time. An implementation of VRIP with minor extensions called VCG is conveniently included in \n        <a href=\"meshlab.net\"> MeshLab </a>[7], and with a subsequent final step of applying <a href=\"https://igl.ethz.ch/projects/instant-meshes/\">Instant Meshes</a> [8] for compression and clean-up, we get the following quite usable result: <br /><br />\n\n\n\n        <div className=\"image center\">\n             <img src={bplant1} width=\"80%\"></img>\n             </div>\n\n        \n\n\n           \n        \n\n\n       \n    <br /><br />\n    <h3> References</h3>\n    [1] Matthew Berger, Andrea Tagliasacchi, Lee Seversky, Pierre Alliez, Gael Guennebaud, et al.. A Survey of Surface Reconstruction from Point Clouds. <i>Computer Graphics Forum,</i> 2016.<br/>\n    [2] Kazhdan, M., Bolitho, M., Hoppe, H.: Poisson surface reconstruction. <i>Symposium on\nGeometry Processing, The Eurographics Association</i>, 2006.<br />\n    [3] Kazhdan, M., Hoppe, H.: Screened poisson surface reconstruction. <i>ACM Trans. Gr. (TOG)</i>, 2013. <br />\n\n    [4] Turk, G., Levoy, M.: Zippered polygon meshes from range\nimages. <i>Proceedings of the 21st Annual Conference on\nComputer Graphics and Interactive Techniques,</i> 1994.<br/>\n    [5] Meerits, S.; Nozick, V.; Saito, H. Real-time scene\nreconstruction and triangle mesh generation using\nmultiple RGB-D cameras. <i>Journal of Real-Time Image\nProcessing</i>, 2017. <br/>\n    [6] Curless, B., and Levoy, M. A volumetric method\nfor building complex models from range images. <i>SIGGRAPH Proceedings</i>, 1996. <br/>    \n    [7] P. Cignoni, M. Callieri, M. Corsini, M. Dellepiane, F. Ganovelli, G. Ranzuglia\nMeshLab: an Open-Source Mesh Processing Tool.\n<i> Sixth Eurographics Italian Chapter Conference</i>, 2008. <br/>\n    [8] W. Jakob, M. Tarini, D. Panozzo, and O. Sorkine-Hornung.\nInstant field-aligned meshes. <i>ACM Transactions on Graphics</i>, Oct. 2015. <br/>\n\n\n\n\n<hr />\n\n\n        <h2>Detecting Mines in the Ocean</h2>\n\n        While working with <a href=\"http://e-cortex.com\">e-Cortex</a>, I had the pleasure of working alongside computer scientists and cognitive computational \n        neuroscientists, and together delving into the problem of detecting explosive ocean mines in the ocean, using\n        video and sonar data collected by Archerfish: unmanned submarines used to find and\n        destroy sea mines. \n        \n        <br /><br />\n        At the time, a human operator would need to remotely watch the video and sonar feed and guide\n        the archerfish to sweep areas for mines, and ultimately decide when to pull the trigger\n        to destroy a visually confirmed mine. Our task was to automate the mine detection process. In order to do this,\n        we were given several dozen sets of sonar/video feed sequences, each consisting of several minutes of\n        mine-seeking until a mine was eventually spotted. After creating training data by assigning mine/no mine labels to the entire dataset, it was time to train some models.\n\n        <br /><br />\n        We expected that a simple image classification approach would suffice for the \n        video data, since upon visual inspection of the video data the mines were fairly easy to discern with the human eye. \n        Our approach was to utilize the image representations learned by Inception [1], a CNN that had been trained for classifcation on a \n        large natural image dataset, by retraining it on our mine sweeping video data. As expected,\n        we achieved accuracy ratings in the upper 90s. \n\n        <br /><br />\n        However, mines were only visible up close in the video data. To spot mines from farther away, \n        the sonar data would need to be utilized. When we tried employing the same strategy as with the video data,\n        the network performed about as well as a random classification strategy. It was essentially a garbage-in-garbage-out situation. So,\n        the key to achieving accurate results turned out to be two preprocessing steps unique to this domain.\n        \n        <br /><br />\n        First, instead of presenting the sonar images in polar coordinates to the network, \n        we transformed them to cartesian space, which allows the convolutional structures and features captured by the network\n        to operate on images more closely resembling the natural images it was pre-trained on.  \n        <br /><br />\n        Second, we utilized pose information of the submarine to align sequences of sonar images\n        instead of looking at them in isolation. By doing this, we were able to drastically reduce the \n        noise present in the sonar data.\n        <br />\n        <br />\n        Taken together, these two preprocessing steps allowed us to train the network to achieve accuracy in the low 90s for the sonar\n        data.\n\n        <br />\n        <br />\n        <h3>References</h3>\n        [1] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and\nA. Rabinovich. Going deeper with convolutions. <i>CVPR,</i> 2015.\n        <hr />\n\n        \n\n\n        \n        {/* <h2>Designing Intelligently Inquisitive Agents</h2>\n\n        \n        <hr /> */}\n\n      \n\n      </div>\n    </Layout>\n  )\n}","module.exports = __webpack_public_path__ + \"static/bplant_overlap-d99ae8259f6adc685b353796c3e7828e.png\";","module.exports = __webpack_public_path__ + \"static/bplant_0-1a9187212fc88c9396aba7d2c6586fee.png\";","module.exports = __webpack_public_path__ + \"static/chair_1-1d3fe6ecd906772e722aa9d9e5cebb47.png\";","module.exports = __webpack_public_path__ + \"static/bplant_2-92f1c0204586ef336c69fc3e92bca2ce.png\";","module.exports = __webpack_public_path__ + \"static/chair_base-4ac9b933aa2b6931c3eec1d6362260b5.png\";","module.exports = __webpack_public_path__ + \"static/bplant_1-d184342f5a18d86bbdb4fc68a20b9d72.png\";","module.exports = __webpack_public_path__ + \"static/bplant_base-5ca6c7365545c56fe033c751d36bb778.png\";","module.exports = __webpack_public_path__ + \"static/chair_0-763b2c4f9258a7d4a6b68863b1cec7d3.png\";"],"sourceRoot":""}