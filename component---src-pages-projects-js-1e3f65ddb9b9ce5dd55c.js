(window.webpackJsonp=window.webpackJsonp||[]).push([[7],{"16l3":function(e,t,a){"use strict";a.r(t),a.d(t,"default",(function(){return p}));var n=a("q1tI"),o=a.n(n),i=a("TJpk"),r=a.n(i),l=a("Bl7J"),s=a("PEjc"),c=a.n(s),h=(a("bjUg"),a("vGzP")),u=a.n(h),m=a("BPjR"),d=a.n(m);a("AF30"),a("S61Z"),a("I9tU");function p(){return o.a.createElement(l.a,null,o.a.createElement(r.a,null,o.a.createElement("title",null,"Rob Cohn's page"),o.a.createElement("meta",{name:"description",content:"Rob Cohn"})),o.a.createElement("div",{id:"main"},o.a.createElement("h1",null,"Some Past Projects"),o.a.createElement("hr",null),o.a.createElement("h2",null,"Autonomous Robotic 3D Scanning for Photorealistic Models of Objects"),o.a.createElement("div",{className:"image left"},o.a.createElement("img",{src:c.a,className:"left"})),o.a.createElement("br",null),o.a.createElement("br",null),"In the context of 3D object scanning, the goal is to measure a 3D object like the brown chair to the left with a physical scanning device, and then process the data collected to construct a model of the object: an asset which can be used in a variety of digital media such as movies, games, or online shopping interfaces.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("div",{className:"image right"},o.a.createElement("img",{src:u.a,className:"right"})),"The first step of this process is to collect data about the object from all necessary perspectives, which might produce a pointcloud like that shown on the right. This is currently done with hand-held scanning devices, making the process labor-intensive and time-consuming, which limits the number of objects companies can scan. ",o.a.createElement("br",null),o.a.createElement("br",null),"I'm currently working with 3co on developing an autonomous robotic solution that will offer a way to obtain photorealistic 3D scans of physical objects at the push of a button.",o.a.createElement("div",{className:"image left"},o.a.createElement("img",{src:d.a,className:"left"})),o.a.createElement("br",null),o.a.createElement("br",null),"For those interested, I dive into a little more detail about just the surface reconstruction part of the problem ",o.a.createElement("a",{href:"personal_site/surfacereconstruction"},"here"),". That is, given a set of range scans in the form of point clouds representing different views of the object, how that can all be neatly integrated into a single coherent mesh like the one on the left.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("hr",null),o.a.createElement("h2",null,"Detecting Mines in the Ocean"),"While working with ",o.a.createElement("a",{href:"http://e-cortex.com"},"e-Cortex"),", I had the pleasure of working alongside computer scientists and cognitive computational neuroscientists, and together delving into the problem of detecting explosive ocean mines in the ocean, using video and sonar data collected by Archerfish: unmanned submarines used to find and destroy sea mines.",o.a.createElement("br",null),o.a.createElement("br",null),"At the time, a human operator would need to remotely watch the video and sonar feed and guide the archerfish to sweep areas for mines, and ultimately decide when to pull the trigger to destroy a visually confirmed mine. Our task was to automate the mine detection process. In order to do this, we were given several dozen sets of sonar/video feed sequences, each consisting of several minutes of mine-seeking until a mine was eventually spotted. After creating training data by assigning mine/no mine labels to the entire dataset, it was time to train some models.",o.a.createElement("br",null),o.a.createElement("br",null),"We expected that a simple image classification approach would suffice for the video data, since upon visual inspection of the video data the mines were fairly easy to discern with the human eye. Our approach was to utilize the image representations learned by Inception [1], a CNN that had been trained for classifcation on a large natural image dataset, by retraining it on our mine sweeping video data. As expected, we achieved accuracy ratings in the upper 90s.",o.a.createElement("br",null),o.a.createElement("br",null),"However, mines were only visible up close in the video data. To spot mines from farther away, the sonar data would need to be utilized. When we tried employing the same strategy as with the video data, the network performed about as well as a random classification strategy. It was essentially a garbage-in-garbage-out situation. So, the key to achieving accurate results turned out to be two preprocessing steps unique to this domain.",o.a.createElement("br",null),o.a.createElement("br",null),"First, instead of presenting the sonar images in polar coordinates to the network, we transformed them to cartesian space, which allows the convolutional structures and features captured by the network to operate on images more closely resembling the natural images it was pre-trained on.",o.a.createElement("br",null),o.a.createElement("br",null),"Second, we utilized pose information of the submarine to align sequences of sonar images instead of looking at them in isolation. By doing this, we were able to drastically reduce the noise present in the sonar data.",o.a.createElement("br",null),o.a.createElement("br",null),"Taken together, these two preprocessing steps allowed us to train the network to achieve accuracy in the low 90s for the sonar data.",o.a.createElement("br",null),o.a.createElement("br",null),o.a.createElement("h2",null,"References"),"[1] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. ",o.a.createElement("i",null,"CVPR,")," 2015.",o.a.createElement("hr",null)))}},AF30:function(e,t,a){e.exports=a.p+"static/bplant_0-1a9187212fc88c9396aba7d2c6586fee.png"},BPjR:function(e,t,a){e.exports=a.p+"static/chair_1-1d3fe6ecd906772e722aa9d9e5cebb47.png"},I9tU:function(e,t,a){e.exports=a.p+"static/bplant_2-92f1c0204586ef336c69fc3e92bca2ce.png"},PEjc:function(e,t,a){e.exports=a.p+"static/chair_base-4ac9b933aa2b6931c3eec1d6362260b5.png"},S61Z:function(e,t,a){e.exports=a.p+"static/bplant_1-d184342f5a18d86bbdb4fc68a20b9d72.png"},bjUg:function(e,t,a){e.exports=a.p+"static/bplant_base-5ca6c7365545c56fe033c751d36bb778.png"},vGzP:function(e,t,a){e.exports=a.p+"static/chair_0-763b2c4f9258a7d4a6b68863b1cec7d3.png"}}]);
//# sourceMappingURL=component---src-pages-projects-js-1e3f65ddb9b9ce5dd55c.js.map